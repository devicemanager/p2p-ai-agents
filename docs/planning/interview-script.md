# User Interview Script - P2P AI Agents MVP

**Project**: P2P AI Agents  
**Sprint**: Week 1 - Discovery & Definition  
**Owner**: John (Product Manager)  
**Created**: 2026-01-10  
**Story**: #9 - Conduct User Interviews

---

## Interview Objectives

1. Validate problem hypothesis: Need for decentralized AI compute
2. Understand current pain points with existing solutions
3. Identify target user personas and use cases
4. Gauge trust/security concerns with P2P networks
5. Score problem validation: 4/5+ say "this is a real problem"

---

## Interview Structure (30-45 minutes)

### Opening (5 min)

**Introduction:**
"Hi [Name], thanks for taking the time. I'm exploring how developers and researchers currently handle distributed AI workloads and compute challenges. This is purely research - no product pitch. Your honest feedback helps validate whether we're solving a real problem. Sound good?"

**Consent:**
- "Mind if I take notes?"
- "Anything you share stays confidential"
- "Feel free to pass on any question"

---

### Part 1: Current State & Pain Points (15 min)

**Q1: Background**
"Tell me about your role and how you currently use AI/ML in your work?"
- *Listen for: Job role, technical level, AI usage frequency*

**Q2: Distributed Computing Challenges**
"What challenges do you face with compute resources for AI workloads?"
- *Probe: Cost? Availability? Control? Performance?*
- *Listen for: Specific pain points, workarounds*

**Q3: Current Solutions**
"What are you using today to solve these challenges?"
- *Examples: AWS, GCP, own servers, nothing?*
- *Probe: What works? What doesn't? Why?*

**Q4: Cost & Scale**
"How much are you spending on compute? Is it a blocker for what you want to do?"
- *Listen for: Budget constraints, scaling issues*

**Q5: Frustrations**
"If you could wave a magic wand and fix one thing about your current setup, what would it be?"
- *This often reveals the REAL pain*

---

### Part 2: Solution Validation (15 min)

**Concept Introduction:**
"Let me describe a concept and get your reaction. Imagine a peer-to-peer network where developers can contribute idle compute (their PCs, servers) and access distributed AI processing without centralized cloud providers. Think BOINC meets AI agents."

**Q6: Initial Reaction**
"What's your gut reaction to this idea?"
- *Listen for: Excitement? Skepticism? Confusion?*

**Q7: Trust & Security**
"Would you trust running your AI workloads on a P2P network? What concerns do you have?"
- *Probe: Data privacy, reliability, security, compliance*

**Q8: Use Cases**
"What specific tasks or workloads would you consider running on something like this?"
- *Listen for: Concrete use cases, task types*

**Q9: Barriers to Adoption**
"What would prevent you from using something like this?"
- *Listen for: Deal-breakers, requirements, must-haves*

**Q10: Willingness to Contribute**
"Would you contribute your own idle compute to such a network? Why or why not?"
- *Listen for: Incentives needed, concerns*

---

### Part 3: Prioritization (5 min)

**Q11: Feature Priorities**
"If this existed, which is most important to you?" (rank 1-5)
- [ ] Low cost vs cloud providers
- [ ] Data privacy & security
- [ ] Easy setup/integration
- [ ] Performance & reliability
- [ ] Community/open-source ethos

**Q12: Problem Severity**
"On a scale of 1-10, how big is the problem we're trying to solve?"
- 1 = Not a problem
- 10 = Critical blocker preventing progress
- *This is our validation metric*

---

### Closing (5 min)

**Q13: Alternatives Considered**
"If this didn't exist, what would you do instead?"
- *Listen for: Acceptable alternatives vs must-solve*

**Q14: Network Effects**
"Who else in your network faces similar challenges?"
- *Potential for referrals, validation of market*

**Q15: Follow-up**
"Would you be willing to see a demo once we have something working?"
- *Gauge genuine interest vs polite responses*

**Thank You:**
"This was incredibly helpful. Can I reach out with follow-up questions?"

---

## Post-Interview: Scoring & Synthesis

### Problem Validation Score
Rate interview on problem validation:
- **5**: "This is critical - I'd pay/use immediately"
- **4**: "Real problem, interested to see solution"
- **3**: "Nice to have, not urgent"
- **2**: "Interesting concept, no personal need"
- **1**: "Not a problem for me"

**Target**: 4/5+ average across 5 interviews = validated problem

### Key Themes to Track
- [ ] Cost concerns mentioned
- [ ] Trust/security concerns mentioned
- [ ] Performance requirements specified
- [ ] Specific use cases identified
- [ ] Would contribute compute
- [ ] Would use network
- [ ] Competitive alternatives named

### Persona Indicators
Track evidence for persona definition:
- Job role & technical level
- Company size & type
- AI/ML workload characteristics
- Budget constraints
- Decision authority
- Tool preferences

---

## Target Interview Candidates (10 total)

### Cohort 1: AI/ML Researchers (3)
- University researchers with limited budgets
- Independent ML engineers
- AI startup founders

### Cohort 2: DevOps/Infrastructure (3)
- Platform engineers managing compute
- SREs dealing with scaling
- Cloud architects optimizing costs

### Cohort 3: Independent Developers (2)
- Solo developers with ML side projects
- Open-source AI contributors
- Hobbyists with compute constraints

### Cohort 4: Technical Founders (2)
- Startup CTOs building AI products
- Technical co-founders with scaling challenges
- Early-stage companies pre-Series A

---

## Success Criteria (Story #9 AC)

- [ ] 5+ interviews completed (30-45 min each)
- [ ] Interview notes captured for each session
- [ ] Common themes synthesized
- [ ] Problem validation score: 4/5+ average
- [ ] Primary user persona identified
- [ ] At least 3 concrete use cases documented

---

## Interview Logistics

**Scheduling:**
- Send calendar invite with Zoom/meet link
- Include agenda/topics overview
- Offer 2-3 time options
- Confirm 24h before

**Tools:**
- Note-taking: Markdown doc per interview
- Recording: Ask permission (optional)
- Synthesis: Aggregate findings in project-context.md

**Timeline:**
- Days 2-3 (Jan 11-12): Candidate outreach
- Days 3-5 (Jan 12-14): Conduct interviews
- Day 6 (Jan 15): Synthesis

---

**Script Version**: 1.0  
**Owner**: John (PM)  
**Status**: Ready for use  
**Next**: Identify and invite candidates
